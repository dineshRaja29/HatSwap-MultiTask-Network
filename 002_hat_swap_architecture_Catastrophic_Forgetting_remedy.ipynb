{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshRaja29/SpeechArchitectures_Hat-Swap-Architecture/blob/main/002_hat_swap_architecture_Catastrophic_Forgetting_remedy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "zU8xJyvJrXKS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <font color = 'green'><b>GOAL:</b></font>"
      ],
      "metadata": {
        "id": "bSc2ZCctCAeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Build a Hat Swap network for different datasets and train model in such a way that we can avoid \"Catastrophic\n",
        "Forgetting\""
      ],
      "metadata": {
        "id": "l8Lfje3uCOJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'green'><b>Hat Swap Network:</b></font> Network with an output layer for each task (or dataset), but shared hidden layers.\n",
        "\n",
        "<font color = 'green'><b>Catastrophic Forgetting: </b></font> When we sequentially train a shared model on multiple tasks, each new training phase overwrites the learned parameters, especially in the shared layers. This leads to the model performing well on the last-trained language, but forgetting previous ones.\n",
        "\n",
        "* Reference:\n",
        "    * https://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr14-multiling.pdf\n",
        "    * https://www.ibm.com/think/topics/catastrophic-forgetting"
      ],
      "metadata": {
        "id": "mrrd7A6qChhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>DATASET</b></font>"
      ],
      "metadata": {
        "id": "VhD_yKG8DHUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Considering the CIFAR10 dataset as our base dataset\n",
        "* From CIFAR10, created three task's dataset\n",
        "    * with one: All images with label as 1 considered as 1 and images with label as 4 and 7 are considered as 0\n",
        "    * with two: All images with label as 2 considered as 1 and images with label as 5 and 6 are considered as 0\n",
        "    * with three: All images with label as 3 considered as 1 and images with label as 8 and 9 are considered as 0\n",
        "* Merge these three task's dataset with an additional column which store the task identification."
      ],
      "metadata": {
        "id": "b6ivoxFREaRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>DATASET PREPARATION</b></font>"
      ],
      "metadata": {
        "id": "snZSDoHAFG2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoFeatureExtractor, Dinov2Model\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "# note: PIL stands for pillow; to install type \"pip3 install pillow\"\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from torchvision.transforms import Compose, Resize, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, gc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:37:46.986703Z",
          "iopub.execute_input": "2025-07-30T02:37:46.986908Z",
          "iopub.status.idle": "2025-07-30T02:38:12.016847Z",
          "shell.execute_reply.started": "2025-07-30T02:37:46.986892Z",
          "shell.execute_reply": "2025-07-30T02:38:12.016221Z"
        },
        "id": "oxibrPd5rXKV",
        "outputId": "0347ce0b-cf2e-429e-992f-ec7da4b301c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-07-30 02:37:58.315812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753843078.494355      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753843078.547182      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple transformation\n",
        "transform = transforms.ToTensor()\n",
        "to_pil = ToPILImage()\n",
        "# download data\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# intermediate directories to save data\n",
        "save_root = '/content/drive/MyDrive/cifar10_binary'\n",
        "os.makedirs(save_root, exist_ok = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:38:12.017443Z",
          "iopub.execute_input": "2025-07-30T02:38:12.017886Z",
          "iopub.status.idle": "2025-07-30T02:38:19.179776Z",
          "shell.execute_reply.started": "2025-07-30T02:38:12.017868Z",
          "shell.execute_reply": "2025-07-30T02:38:19.179164Z"
        },
        "id": "HL8QP06-rXKu",
        "outputId": "39d6d646-ef22-468a-a730-19445e6062cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 170M/170M [00:03<00:00, 46.7MB/s] \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def label_adjustment(dataset, pos, neg):\n",
        "    results = []\n",
        "    for img, label in dataset:\n",
        "        if label == pos:\n",
        "            results.append([img, 1])\n",
        "        if label in neg:\n",
        "             results.append([img, 0])\n",
        "    return results\n",
        "\n",
        "def save_images_and_make_csv(data, split_name):\n",
        "    dir = os.path.join(save_root, split_name)\n",
        "    os.makedirs(dir, exist_ok = True)\n",
        "    rows = []\n",
        "    for idx, (img_tensor, label) in enumerate(data):\n",
        "        img_path = os.path.join(dir, f'{idx}.png')\n",
        "        to_pil(img_tensor).save(img_path)\n",
        "        rows.append([img_path, label])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns = [\"MD5HASH\", \"LABEL\"])\n",
        "    df.to_csv(os.path.join(save_root, f\"{split_name}.csv\"), index = False)\n",
        "    print(f\"{split_name}.csv saved with {len(rows)} entries.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:38:37.213223Z",
          "iopub.execute_input": "2025-07-30T02:38:37.213496Z",
          "iopub.status.idle": "2025-07-30T02:38:37.219585Z",
          "shell.execute_reply.started": "2025-07-30T02:38:37.213476Z",
          "shell.execute_reply": "2025-07-30T02:38:37.218789Z"
        },
        "id": "7NuAq7vGrXKz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "             'with_one': [1, [4,7]],\n",
        "             'with_two': [2, [5,6]],\n",
        "             'with_three': [3, [8,9]]\n",
        "            }\n",
        "def generate_multiple_csv_files():\n",
        "\n",
        "    for k, v in labels.items():\n",
        "        train_data = label_adjustment(train_set, v[0], v[1])\n",
        "        test_data  = label_adjustment(test_set, v[0], v[1])\n",
        "        # Save both splits\n",
        "        save_images_and_make_csv(train_data, f\"{k}_train\")\n",
        "        save_images_and_make_csv(test_data, f\"{k}_test\")\n",
        "\n",
        "generate_multiple_csv_files()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:38:43.103955Z",
          "iopub.execute_input": "2025-07-30T02:38:43.104476Z",
          "iopub.status.idle": "2025-07-30T02:39:27.883203Z",
          "shell.execute_reply.started": "2025-07-30T02:38:43.104453Z",
          "shell.execute_reply": "2025-07-30T02:39:27.882441Z"
        },
        "id": "kEmUUUhrrXK6",
        "outputId": "8467e16a-04dd-45a8-98b4-4368f6dcd53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "with_one_train.csv saved with 15000 entries.\nwith_one_test.csv saved with 3000 entries.\nwith_two_train.csv saved with 15000 entries.\nwith_two_test.csv saved with 3000 entries.\nwith_three_train.csv saved with 15000 entries.\nwith_three_test.csv saved with 3000 entries.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "languages_dataset = []\n",
        "for data in labels.keys():\n",
        "    file = f\"/content/drive/MyDrive/cifar10_binary/{data}_train.csv\"\n",
        "    frame = pd.read_csv(file)\n",
        "    frame['LANGUAGE'] = data\n",
        "    languages_dataset.append(frame)\n",
        "\n",
        "# Concatenate all language-specific DataFrames into one\n",
        "full_dataset = pd.concat(languages_dataset, ignore_index=True)\n",
        "full_dataset.to_csv('/content/drive/MyDrive/cifar10_binary/all_languages_training_data.csv', index = False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:39:27.884501Z",
          "iopub.execute_input": "2025-07-30T02:39:27.884728Z",
          "iopub.status.idle": "2025-07-30T02:39:28.049552Z",
          "shell.execute_reply.started": "2025-07-30T02:39:27.884711Z",
          "shell.execute_reply": "2025-07-30T02:39:28.048793Z"
        },
        "id": "3nCEFTEurXK8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/cifar10_binary/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:39:28.050368Z",
          "iopub.execute_input": "2025-07-30T02:39:28.050618Z",
          "iopub.status.idle": "2025-07-30T02:39:28.207143Z",
          "shell.execute_reply.started": "2025-07-30T02:39:28.050591Z",
          "shell.execute_reply": "2025-07-30T02:39:28.206051Z"
        },
        "id": "4uBrm5YcrXLC",
        "outputId": "45127285-e3c2-480a-a402-192b01b4443b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "all_languages_training_data.csv  with_three_test       with_two_test.csv\nwith_one_test\t\t\t with_three_test.csv   with_two_train\nwith_one_test.csv\t\t with_three_train      with_two_train.csv\nwith_one_train\t\t\t with_three_train.csv\nwith_one_train.csv\t\t with_two_test\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pDuIpP7IrXLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wc -l /content/drive/MyDrive/cifar10_binary/all_languages_training_data.csv"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:39:28.209811Z",
          "iopub.execute_input": "2025-07-30T02:39:28.210181Z",
          "iopub.status.idle": "2025-07-30T02:39:28.378868Z",
          "shell.execute_reply.started": "2025-07-30T02:39:28.210142Z",
          "shell.execute_reply": "2025-07-30T02:39:28.378069Z"
        },
        "id": "GL2wgx2yrXLW",
        "outputId": "df3981db-4bb2-4275-fe70-f8ae6f305852"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "45001 /content/drive/MyDrive/cifar10_binary/all_languages_training_data.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /content/drive/MyDrive/cifar10_binary/all_languages_training_data.csv | head -4"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:39:28.379974Z",
          "iopub.execute_input": "2025-07-30T02:39:28.380284Z",
          "iopub.status.idle": "2025-07-30T02:39:28.550766Z",
          "shell.execute_reply.started": "2025-07-30T02:39:28.380252Z",
          "shell.execute_reply": "2025-07-30T02:39:28.549996Z"
        },
        "id": "ebYjVoNprXLb",
        "outputId": "78e08849-a8c5-4a9e-fb86-bb4196c9b87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "MD5HASH,LABEL,LANGUAGE\n/content/drive/MyDrive/cifar10_binary/with_one_train/0.png,0,with_one\n/content/drive/MyDrive/cifar10_binary/with_one_train/1.png,1,with_one\n/content/drive/MyDrive/cifar10_binary/with_one_train/2.png,1,with_one\ncat: write error: Broken pipe\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>SETTING ENVIRONMENT FOR HARDWARE ACCELERATOR</b></font>"
      ],
      "metadata": {
        "id": "T8nBknmUFaCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for CUDA and MPS availability, set the device accordingly\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    # setting environment variables, need to run training in MacOS\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
        "    os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
        "    print(\"Using MPS as the device.\")\n",
        "else:\n",
        "    if torch.cuda.is_available():\n",
        "\t# the syntax 'cuda:3' used to point a specific GPU from the cluster of GPUs\n",
        "\t# 'cuda' points to first GPU from the cluster of GPUs\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"Using CUDA as the device.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Using CPU as the device.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:39:56.769339Z",
          "iopub.execute_input": "2025-07-30T02:39:56.769612Z",
          "iopub.status.idle": "2025-07-30T02:39:56.775933Z",
          "shell.execute_reply.started": "2025-07-30T02:39:56.769591Z",
          "shell.execute_reply": "2025-07-30T02:39:56.775138Z"
        },
        "id": "ZI0BaabfrXLl",
        "outputId": "119564b1-adc2-4501-8334-ff61961f448d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using CUDA as the device.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>CONFIGURATION VARIABLES AND UTILITY FUNCTIONS</b></font>"
      ],
      "metadata": {
        "id": "KwocvnsNFfbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE                            = 128 #256\n",
        "WORKERS                               = 4\n",
        "PIN_MEMORY                            = True\n",
        "MIXING                                = True\n",
        "MODEL_NAME                            = \"facebook/dinov2-base\"\n",
        "RESULTS                               = 'results'\n",
        "EPOCHS                                = 5\n",
        "BEST_MODEL                            = None\n",
        "PRETRAINING                           = False\n",
        "LEARNING_RATE                         = 1e-4\n",
        "L2_PENALTY                            = 1e-4\n",
        "GAMMA                                 = 0.1\n",
        "STEPSIZE                              = 3\n",
        "SAVE_CHECKPOINTS                      = True\n",
        "MIN_LOSS                              = float('inf')\n",
        "MODEL_SAVED                           = f'{RESULTS}/bestmodel.pth'\n",
        "THRESHOLD                             = 0.5\n",
        "OUTPUT_DIM                            = 1\n",
        "HEADS                                 = ['with_one', 'with_two', 'with_three']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:00.937613Z",
          "iopub.execute_input": "2025-07-30T02:40:00.937918Z",
          "iopub.status.idle": "2025-07-30T02:40:00.943001Z",
          "shell.execute_reply.started": "2025-07-30T02:40:00.937897Z",
          "shell.execute_reply": "2025-07-30T02:40:00.942209Z"
        },
        "id": "XkU-fIkFrXLx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_classification_accuracy(loader, model, head):\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "    LABELS = []\n",
        "    PREDICTIONS = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            # Move to device and cast to float32\n",
        "            images, labels = images.to(device), labels.to(device).float()\n",
        "            features = model(images).squeeze()\n",
        "            probabilities = model.classify(features, head)\n",
        "            # Predictions based on the threshold\n",
        "            prediction = torch.where(probabilities > THRESHOLD, 1.0, 0.0)\n",
        "            LABELS.extend(labels.tolist())\n",
        "            PREDICTIONS.extend(prediction.tolist())\n",
        "    return classification_report(LABELS, PREDICTIONS)\n",
        "\n",
        "\n",
        "def cleaning_memory():\n",
        "    # Explicitly free up GPU memory\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.backends.mps.is_macos13_or_newer.cache_clear()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    # Run garbage collector to free up CPU memory\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:04.17797Z",
          "iopub.execute_input": "2025-07-30T02:40:04.178664Z",
          "iopub.status.idle": "2025-07-30T02:40:04.18439Z",
          "shell.execute_reply.started": "2025-07-30T02:40:04.178641Z",
          "shell.execute_reply": "2025-07-30T02:40:04.183608Z"
        },
        "id": "kuWvvfCPrXLy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>MODEL ARCHITECTURE</b></font>\n",
        "> Taking DINO as feature extractor which we will fine-tune on our dataset."
      ],
      "metadata": {
        "id": "kpUeVJX3FuKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadNetwork(nn.Module):\n",
        "    # Reference: https://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr14-multiling.pdf\n",
        "    def __init__(self, heads):\n",
        "        super(MultiHeadNetwork, self).__init__()\n",
        "\n",
        "        # Load pre-trained processor and backbone model\n",
        "        # taking processor for necessary substitions, if needed in later stages\n",
        "        self.processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
        "        self.backbone_model = Dinov2Model.from_pretrained(MODEL_NAME)\n",
        "        self.pretrained_backbone_model_last_dim = self.backbone_model.layernorm.normalized_shape[0]\n",
        "\n",
        "        # Create a dictionary to hold the heads\n",
        "        self.heads = nn.ModuleDict()\n",
        "\n",
        "        for head in heads:\n",
        "            # Create a classification head for each entry in heads\n",
        "            self.heads[head] = nn.Sequential(\n",
        "                nn.Linear(self.pretrained_backbone_model_last_dim, OUTPUT_DIM, bias=True),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        torch.manual_seed(444)\n",
        "        for head_name, head in self.heads.items():\n",
        "            for layer in head:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "                    print(f\"kaiming_uniform_ Initialization: {layer.__class__.__name__}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward only through the shared backbone.\n",
        "        Language-specific heads are applied outside in training loop.\n",
        "        Acts as a Feature Extractor\n",
        "        \"\"\"\n",
        "        features = self.backbone_model(x).last_hidden_state[:, 0]\n",
        "        return features\n",
        "\n",
        "    def classify(self, features, lang):\n",
        "        \"\"\"\n",
        "        Forward through a specific language head.\n",
        "        \"\"\"\n",
        "        return self.heads[lang](features)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:10.339645Z",
          "iopub.execute_input": "2025-07-30T02:40:10.340502Z",
          "iopub.status.idle": "2025-07-30T02:40:10.352554Z",
          "shell.execute_reply.started": "2025-07-30T02:40:10.340465Z",
          "shell.execute_reply": "2025-07-30T02:40:10.351695Z"
        },
        "id": "q5kL9vlzrXLz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = MultiHeadNetwork(HEADS)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:15.233833Z",
          "iopub.execute_input": "2025-07-30T02:40:15.234554Z",
          "iopub.status.idle": "2025-07-30T02:40:20.008776Z",
          "shell.execute_reply.started": "2025-07-30T02:40:15.234521Z",
          "shell.execute_reply": "2025-07-30T02:40:20.008002Z"
        },
        "id": "hav3fT42rXL4",
        "outputId": "1b2bdfa0-cf2f-4d30-d88f-41923911d4b2",
        "colab": {
          "referenced_widgets": [
            "7c358e5f28e94b468a8989fa28b077b7",
            "ec9b323951004b2fb32ab90f3f5ae453",
            "5518dd8ae7444830a9583001bb49263a",
            "342bee739f69499f9380426f68f4133e",
            "2ea1df93edc3403ebaeab0d0232f24b1",
            "9d83378b23ab4764b213bc753e01261b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "342bee739f69499f9380426f68f4133e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ea1df93edc3403ebaeab0d0232f24b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d83378b23ab4764b213bc753e01261b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "kaiming_uniform_ Initialization: Linear\nkaiming_uniform_ Initialization: Linear\nkaiming_uniform_ Initialization: Linear\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "MultiHeadNetwork(\n  (backbone_model): Dinov2Model(\n    (embeddings): Dinov2Embeddings(\n      (patch_embeddings): Dinov2PatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Dinov2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x Dinov2Layer(\n          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (attention): Dinov2Attention(\n            (attention): Dinov2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Dinov2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (layer_scale1): Dinov2LayerScale()\n          (drop_path): Identity()\n          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Dinov2MLP(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (activation): GELUActivation()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_scale2): Dinov2LayerScale()\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): ModuleDict(\n    (with_one): Sequential(\n      (0): Linear(in_features=768, out_features=1, bias=True)\n      (1): Sigmoid()\n    )\n    (with_two): Sequential(\n      (0): Linear(in_features=768, out_features=1, bias=True)\n      (1): Sigmoid()\n    )\n    (with_three): Sequential(\n      (0): Linear(in_features=768, out_features=1, bias=True)\n      (1): Sigmoid()\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>DATASET LOADING</b></font>"
      ],
      "metadata": {
        "id": "cp-QumRrF_h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MD5HASHDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.images = self.dataframe['MD5HASH'].values\n",
        "        self.labels = self.dataframe['LABEL'].values\n",
        "        try:\n",
        "            self.languages = self.dataframe['LANGUAGE'].values\n",
        "        except:\n",
        "            pass\n",
        "        self.processor = model.processor\n",
        "        self.mean = self.processor.image_mean\n",
        "        self.std = self.processor.image_std\n",
        "        self.interpolation = self.processor.resample\n",
        "\n",
        "        self.train_transform = Compose([\n",
        "            Resize(size = (32, 32)),\n",
        "            #RandomResizedCrop(size = (224, 224),\n",
        "            #                  scale = (0.08, 1.0),\n",
        "            #                  ratio = (0.75, 1.3333),\n",
        "            #                  interpolation = self.interpolation),\n",
        "            #RandomHorizontalFlip(p = 0.5),\n",
        "            #ColorJitter(brightness = (0.6, 1.4),\n",
        "            #            contrast = (0.6, 1.4),\n",
        "            #            saturation = (0.6, 1.4)),\n",
        "            ToTensor(),\n",
        "            Normalize(mean = self.mean, std = self.std),\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image from the file path\n",
        "        image_path = self.images[idx]\n",
        "        image = self.train_transform(Image.open(image_path).convert('RGB'))\n",
        "        # Get the label\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        try:\n",
        "            language = self.languages[idx]\n",
        "        except:\n",
        "            return image, label\n",
        "        return image, label, language\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:40.69219Z",
          "iopub.execute_input": "2025-07-30T02:40:40.692773Z",
          "iopub.status.idle": "2025-07-30T02:40:40.699577Z",
          "shell.execute_reply.started": "2025-07-30T02:40:40.692734Z",
          "shell.execute_reply": "2025-07-30T02:40:40.698978Z"
        },
        "id": "a9vuzRxFrXL6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_loader(data_csv, upsampling = False):\n",
        "    # Load data\n",
        "    training_data = pd.read_csv(data_csv)\n",
        "    print('::: DATA DETAILS :::')\n",
        "    print('- Number of Samples:', training_data.shape[0])\n",
        "    # Create dataset and dataloader\n",
        "    md5hash_dataset = MD5HASHDataset(training_data)\n",
        "    if upsampling:\n",
        "        # References:\n",
        "        # https://pytorch.org/docs/stable/data.html\n",
        "        # https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452\n",
        "        from torch.utils.data import WeightedRandomSampler\n",
        "        print('- LANGUAGE DISTRIBUTION: \\n',training_data['LANGUAGE'].value_counts())\n",
        "        classes_count = dict(training_data['LANGUAGE'].value_counts())\n",
        "        sample_weights = [ 1 / classes_count[i] for i in training_data.LANGUAGE.values]\n",
        "        sampler = WeightedRandomSampler(weights = sample_weights,\n",
        "                                        num_samples = len(training_data),\n",
        "                                        replacement = True)\n",
        "        data_loader = DataLoader(md5hash_dataset,\n",
        "                                 batch_size = BATCH_SIZE,\n",
        "                                 num_workers = WORKERS,\n",
        "                                 pin_memory = PIN_MEMORY,\n",
        "                                 shuffle = False,\n",
        "                                 sampler = sampler)\n",
        "    else:\n",
        "        data_loader = DataLoader(md5hash_dataset,\n",
        "                                 batch_size = BATCH_SIZE,\n",
        "                                 num_workers = WORKERS,\n",
        "                                 pin_memory = PIN_MEMORY,\n",
        "                                 shuffle = MIXING)\n",
        "\n",
        "    # Clean memory, :)\n",
        "    del training_data\n",
        "\n",
        "    return data_loader\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:45.003043Z",
          "iopub.execute_input": "2025-07-30T02:40:45.003329Z",
          "iopub.status.idle": "2025-07-30T02:40:45.009337Z",
          "shell.execute_reply.started": "2025-07-30T02:40:45.003308Z",
          "shell.execute_reply": "2025-07-30T02:40:45.008645Z"
        },
        "id": "dhAb3ohOrXL7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "file = f'/content/drive/MyDrive/cifar10_binary/all_languages_training_data.csv'\n",
        "data_loader = create_training_loader(file, upsampling = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:41:11.262606Z",
          "iopub.execute_input": "2025-07-30T02:41:11.263106Z",
          "iopub.status.idle": "2025-07-30T02:41:11.3395Z",
          "shell.execute_reply.started": "2025-07-30T02:41:11.263082Z",
          "shell.execute_reply": "2025-07-30T02:41:11.338734Z"
        },
        "id": "ryPPHVo3QebW",
        "outputId": "c57e5e0b-1c10-4ad6-b43f-9f75fbf014d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "::: DATA DETAILS :::\n- Number of Samples: 45000\n- LANGUAGE DISTRIBUTION: \n LANGUAGE\nwith_one      15000\nwith_two      15000\nwith_three    15000\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>LOSS FUNCTION</b></font>"
      ],
      "metadata": {
        "id": "TTNofneKGLAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function: BCE\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:50.218707Z",
          "iopub.execute_input": "2025-07-30T02:40:50.219041Z",
          "iopub.status.idle": "2025-07-30T02:40:50.222842Z",
          "shell.execute_reply.started": "2025-07-30T02:40:50.219021Z",
          "shell.execute_reply": "2025-07-30T02:40:50.222003Z"
        },
        "id": "if--kQNzrXL9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# directory creation\n",
        "os.makedirs(RESULTS, exist_ok = True)\n",
        "if SAVE_CHECKPOINTS:\n",
        "    CHECKPOINTDIR = f'{RESULTS}/checkpoints'\n",
        "    os.makedirs(CHECKPOINTDIR, exist_ok = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:53.676733Z",
          "iopub.execute_input": "2025-07-30T02:40:53.677314Z",
          "iopub.status.idle": "2025-07-30T02:40:53.681568Z",
          "shell.execute_reply.started": "2025-07-30T02:40:53.677292Z",
          "shell.execute_reply": "2025-07-30T02:40:53.68081Z"
        },
        "id": "J4TswASdrXL9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>OPTIMIZER</b></font>"
      ],
      "metadata": {
        "id": "D7DoOCIyGP5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Idea borrowed from Research paper titled as \"Improving Generalization Performance by Switching from Adam to SGD\"\n",
        "if PRETRAINING:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum = 0.9, weight_decay = L2_PENALTY)\n",
        "else:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = L2_PENALTY)\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size = STEPSIZE, gamma = GAMMA)  # Adjust step_size and gamma as needed\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:40:58.104514Z",
          "iopub.execute_input": "2025-07-30T02:40:58.104813Z",
          "iopub.status.idle": "2025-07-30T02:40:58.110317Z",
          "shell.execute_reply.started": "2025-07-30T02:40:58.104792Z",
          "shell.execute_reply": "2025-07-30T02:40:58.109496Z"
        },
        "id": "n0nyvtERrXMM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>MODEL TRAINING</b></font>"
      ],
      "metadata": {
        "id": "pqjSM57WrXMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_LOSS = float('inf')\n",
        "# TRAINING LOOP\n",
        "for epoch in range(EPOCHS):\n",
        "    print('-'*70)\n",
        "    # Define the total number of batches in the loader\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # setting model stage to training\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (images, labels, languages) in enumerate(data_loader):\n",
        "        # shifting on hardware accelator\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()  # Moved this line here to avoid accumulating gradients\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # Forward pass\n",
        "            features = model(images).squeeze()  # Squeeze to remove extra dimensions\n",
        "\n",
        "            unique_langs = list(set(languages))\n",
        "            batch_loss = 0.0\n",
        "\n",
        "            for lang in unique_langs:\n",
        "                # Get indices for current language\n",
        "                lang_indices = [i for i, l in enumerate(languages) if l == lang]\n",
        "                if not lang_indices:\n",
        "                    continue\n",
        "\n",
        "                # Gather corresponding features and labels\n",
        "                lang_feats = features[lang_indices]\n",
        "                lang_labels = labels[lang_indices].unsqueeze(1)\n",
        "\n",
        "                # Forward pass through corresponding head\n",
        "                outputs = model.classify(lang_feats, lang)\n",
        "\n",
        "                # Handle edge cases if output shape is scalar\n",
        "                if outputs.dim() == 0:\n",
        "                    outputs = outputs.unsqueeze(0)\n",
        "\n",
        "                lang_loss = criterion(outputs, lang_labels)\n",
        "\n",
        "                batch_loss += lang_loss\n",
        "\n",
        "            # Backprop\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += batch_loss.item()\n",
        "        cleaning_memory() # cleaning memory\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / (batch_idx + 1)}\")\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    if SAVE_CHECKPOINTS:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "        checkpointmodel = '{}/epoch_{}_{}.pth'.format(CHECKPOINTDIR, epoch + 1, timestamp)\n",
        "        print('Saving checkpoint: ', checkpointmodel)\n",
        "        torch.save(model.state_dict(), checkpointmodel)\n",
        "\n",
        "    # Check if this epoch had the minimum loss\n",
        "    if total_loss < MIN_LOSS:\n",
        "        MIN_LOSS = total_loss\n",
        "        best_model = model.state_dict()\n",
        "        # Save the best model\n",
        "        if best_model is not None:\n",
        "            print('Saving Best Model: ', MODEL_SAVED)\n",
        "            torch.save(best_model, MODEL_SAVED)\n",
        "\n",
        "################################"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:41:20.604274Z",
          "iopub.execute_input": "2025-07-30T02:41:20.604961Z",
          "iopub.status.idle": "2025-07-30T02:54:36.187581Z",
          "shell.execute_reply.started": "2025-07-30T02:41:20.604936Z",
          "shell.execute_reply": "2025-07-30T02:54:36.186631Z"
        },
        "id": "kXSDpDdkrXMX",
        "outputId": "aac6e28b-d8ff-4fa9-90aa-c729198d76d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------------------------------------\nEpoch 1/5, Loss: 1.3421790943565695\nSaving checkpoint:  results/checkpoints/epoch_1_20250730024405.pth\nSaving Best Model:  results/bestmodel.pth\n----------------------------------------------------------------------\nEpoch 2/5, Loss: 0.6270777339285071\nSaving checkpoint:  results/checkpoints/epoch_2_20250730024647.pth\nSaving Best Model:  results/bestmodel.pth\n----------------------------------------------------------------------\nEpoch 3/5, Loss: 0.42355297366157174\nSaving checkpoint:  results/checkpoints/epoch_3_20250730024923.pth\nSaving Best Model:  results/bestmodel.pth\n----------------------------------------------------------------------\nEpoch 4/5, Loss: 0.1799721929339946\nSaving checkpoint:  results/checkpoints/epoch_4_20250730025200.pth\nSaving Best Model:  results/bestmodel.pth\n----------------------------------------------------------------------\nEpoch 5/5, Loss: 0.08867472424489362\nSaving checkpoint:  results/checkpoints/epoch_5_20250730025434.pth\nSaving Best Model:  results/bestmodel.pth\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>MODEL EVALUATION</b></font>"
      ],
      "metadata": {
        "id": "0MqNBrQ5GsTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading best model\n",
        "model.load_state_dict(torch.load(MODEL_SAVED, weights_only = True))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:54:36.189599Z",
          "iopub.execute_input": "2025-07-30T02:54:36.189994Z",
          "iopub.status.idle": "2025-07-30T02:54:36.503891Z",
          "shell.execute_reply.started": "2025-07-30T02:54:36.189961Z",
          "shell.execute_reply": "2025-07-30T02:54:36.503273Z"
        },
        "id": "3hf5aA3UrXMb",
        "outputId": "d00fd808-3a04-40bf-ca27-7b7a1ea0ee7f"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for head in HEADS:\n",
        "    print(f\"** Peformance of Head {head.upper()}**\")\n",
        "    for set_name in ['train', 'test']:\n",
        "        file = f'/content/drive/MyDrive/cifar10_binary/{head}_{set_name}.csv'\n",
        "        print(f\"Dataset: {set_name}\")\n",
        "        data_loader = create_training_loader(file,\n",
        "                                             upsampling = False)\n",
        "        # without upsampling, used to report exact performance on the training data and testing data\n",
        "        print(calculate_classification_accuracy(data_loader, model, head))\n",
        "        del data_loader\n",
        "        cleaning_memory() # cleaning memory"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T02:54:36.504545Z",
          "iopub.execute_input": "2025-07-30T02:54:36.504785Z",
          "iopub.status.idle": "2025-07-30T02:54:52.843913Z",
          "shell.execute_reply.started": "2025-07-30T02:54:36.504759Z",
          "shell.execute_reply": "2025-07-30T02:54:52.843181Z"
        },
        "id": "_nBJNvX0rXMd",
        "outputId": "174f22cb-2e0c-46f4-fad1-cdd0b3cda857"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "** Peformance of Head WITH_ONE**\nDataset: train\n::: DATA DETAILS :::\n- Number of Samples: 15000\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00     10000\n         1.0       1.00      1.00      1.00      5000\n\n    accuracy                           1.00     15000\n   macro avg       1.00      1.00      1.00     15000\nweighted avg       1.00      1.00      1.00     15000\n\nDataset: test\n::: DATA DETAILS :::\n- Number of Samples: 3000\n              precision    recall  f1-score   support\n\n         0.0       0.98      0.99      0.98      2000\n         1.0       0.98      0.95      0.97      1000\n\n    accuracy                           0.98      3000\n   macro avg       0.98      0.97      0.97      3000\nweighted avg       0.98      0.98      0.98      3000\n\n** Peformance of Head WITH_TWO**\nDataset: train\n::: DATA DETAILS :::\n- Number of Samples: 15000\n              precision    recall  f1-score   support\n\n         0.0       0.99      0.99      0.99     10000\n         1.0       0.99      0.98      0.98      5000\n\n    accuracy                           0.99     15000\n   macro avg       0.99      0.99      0.99     15000\nweighted avg       0.99      0.99      0.99     15000\n\nDataset: test\n::: DATA DETAILS :::\n- Number of Samples: 3000\n              precision    recall  f1-score   support\n\n         0.0       0.89      0.92      0.90      2000\n         1.0       0.82      0.77      0.79      1000\n\n    accuracy                           0.87      3000\n   macro avg       0.85      0.84      0.85      3000\nweighted avg       0.87      0.87      0.87      3000\n\n** Peformance of Head WITH_THREE**\nDataset: train\n::: DATA DETAILS :::\n- Number of Samples: 15000\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00     10000\n         1.0       0.99      1.00      0.99      5000\n\n    accuracy                           1.00     15000\n   macro avg       1.00      1.00      1.00     15000\nweighted avg       1.00      1.00      1.00     15000\n\nDataset: test\n::: DATA DETAILS :::\n- Number of Samples: 3000\n              precision    recall  f1-score   support\n\n         0.0       0.97      0.96      0.96      2000\n         1.0       0.92      0.93      0.93      1000\n\n    accuracy                           0.95      3000\n   macro avg       0.94      0.95      0.95      3000\nweighted avg       0.95      0.95      0.95      3000\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'green'><b>OBSERVATION</b></font>"
      ],
      "metadata": {
        "id": "VqMuk9_9HOgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * At each iteration, data from all languages is fed into the network.\n",
        "> * The derivatives for the feature extractor are computed by aggregating the derivatives from all output layers.\n",
        "> * Performance evaluation shows that the model performs well across all tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "g42NZzNZHUir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "j6kEx9EfrXO6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}